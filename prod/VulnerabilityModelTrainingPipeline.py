import pandas as pd
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib

class VulnerabilityModelTrainingPipeline:
    def __init__(self,
                 data_path="../training_data/training_data.csv",
                 model_severity_path="../model/severity_model.pkl",
                 model_confidence_path="../model/confidence_model.pkl",
                 scaler_path="../model/scaler.pkl"):
        """
        Initializes the training pipeline with dataset and output model paths.

        :param data_path: Path to CSV file containing features and targets (severity, confidence).
        :param model_severity_path: Where to save the trained severity classification model.
        :param model_confidence_path: Where to save the trained confidence regression model.
        :param scaler_path: Where to save the feature scaler.
        """
        self.data_path = data_path
        self.model_severity_path = model_severity_path
        self.model_confidence_path = model_confidence_path
        self.scaler_path = scaler_path

        # Models and scaler
        self.scaler = StandardScaler()
        self.model_severity = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model_confidence = RandomForestRegressor(n_estimators=100, random_state=42)

    def load_data(self):
        """
        Loads the dataset from CSV and separates features from targets.
        """
        data = pd.read_csv(self.data_path)
        self.X = data.drop(columns=["severity", "confidence"])
        self.y_severity = data["severity"]
        self.y_confidence = data["confidence"]

    def preprocess(self):
        """
        Scales features to standard normal distribution.
        """
        self.X_scaled = self.scaler.fit_transform(self.X)

    def train_models(self, test_size=0.25):
        """
        Trains the severity classification model and confidence regression model.

        :param test_size: Fraction of data used for testing.
        :return: Accuracy for severity, R² score for confidence.
        """
        X_train_sev, X_test_sev, y_train_sev, y_test_sev = train_test_split(
            self.X_scaled, self.y_severity, test_size=test_size, random_state=42
        )
        self.model_severity.fit(X_train_sev, y_train_sev)
        acc_severity = self.model_severity.score(X_test_sev, y_test_sev)

        X_train_conf, X_test_conf, y_train_conf, y_test_conf = train_test_split(
            self.X_scaled, self.y_confidence, test_size=test_size, random_state=42
        )
        self.model_confidence.fit(X_train_conf, y_train_conf)
        r2_confidence = self.model_confidence.score(X_test_conf, y_test_conf)

        return acc_severity, r2_confidence

    def save_models(self):
        """
        Saves trained models and scaler to disk.
        """
        joblib.dump(self.model_severity, self.model_severity_path)
        joblib.dump(self.model_confidence, self.model_confidence_path)
        joblib.dump(self.scaler, self.scaler_path)

    def run_pipeline(self):
        """
        Executes the full training pipeline end-to-end.
        """
        print("[*] Loading data...")
        self.load_data()

        print("[*] Preprocessing...")
        self.preprocess()

        print("[*] Training models...")
        acc_sev, acc_conf = self.train_models()
        print(f"[+] Severity model accuracy: {acc_sev:.2f}")
        print(f"[+] Confidence model R² score: {acc_conf:.2f}")

        print("[*] Saving models and scaler...")
        self.save_models()
        print(f"[+] Models and scaler saved to:\n    - {self.model_severity_path}\n    - {self.model_confidence_path}\n    - {self.scaler_path}")


if __name__ == "__main__":
    pipeline = VulnerabilityModelTrainingPipeline()
    pipeline.run_pipeline()
